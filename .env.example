# Example .env for docker-compose
# Set HOST_CACHE_DIR to a folder on the host that will be used for model caches.
# On Windows set to something like: C:/Users/YourUser/.cache
HOST_CACHE_DIR=C:/Users/GTM_PLANETARY_RIG 1/.cache

# Optional: Hugging Face token for pyannote
HF_TOKEN=

# GPU settings (if available)
CUDA_VISIBLE_DEVICES=

# ============================================
# LLM Provider Configuration
# ============================================
# Choose between free local Ollama or paid cloud OpenAI
# Options: ollama, openai
LLM_PROVIDER=ollama

# Ollama Configuration (free, runs locally)
# Used when LLM_PROVIDER=ollama
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=llama3.1:8b

# OpenAI Configuration (paid, cloud-based)
# Used when LLM_PROVIDER=openai
# Get API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini

# When to use each provider:
# - ollama: Free, private, runs on your hardware, requires GPU for best performance
# - openai: Paid (~$0.15 per 1M tokens), fast, no local resources needed, internet required

# Secret key for JWT token encoding
SECRET_KEY=your_super_secret_key_here
